# Implementation Plan: Phase I – In-Memory Python Todo CLI

**Branch**: `001-core-todos` | **Date**: 2026-01-01 | **Spec**: [specs/001-core-todos/spec.md](spec.md)
**Input**: Feature specification from `/specs/001-core-todos/spec.md`

## Summary

Build a single-user, in-memory Python CLI Todo application that implements the complete CRUD lifecycle for tasks. The app stores all data in memory (no database, no file persistence) and accepts user input via command-line interface. Phase I establishes the foundation for Phases II–V progression toward a distributed, cloud-native, AI-powered system. Implementation is 100% specification-driven via Claude Code; no manual code changes allowed.

**Primary Requirement**: A functioning CLI Todo app with 5 core features (add, list, complete, update, delete) that directly satisfies all 13 functional requirements and 17 acceptance scenarios from the spec.

**Technical Approach**: Layered architecture separating CLI interaction (argparse-based command parsing) from task logic (TaskManager class) and data structures (Task data class). Single-threaded, synchronous execution sufficient for Phase I; stateless design enables horizontal scaling in Phase IV+.

## Technical Context

**Language/Version**: Python 3.13+
**Primary Dependencies**: Python standard library only (argparse, json, datetime, dataclasses)
**Storage**: In-memory only (list of Task objects in memory)
**Testing**: pytest (industry-standard; not part of stdlib but widely available)
**Target Platform**: Linux, macOS, WSL 2 (Windows); any platform with Python 3.13+
**Project Type**: Single-project CLI application
**Performance Goals**: <10 seconds for typical 3-task workflow; handle 1000+ tasks without slowdown
**Constraints**: No database, no file I/O, no external libraries beyond stdlib, no manual coding
**Scale/Scope**: Single-user, single-session execution; ~10K tasks maximum per session

## Constitution Check

**GATE: Must pass before implementation begins.**

Validating Phase I plan against Hackathon II Constitution (v1.0.0):

| Principle | Status | Validation |
|-----------|--------|-----------|
| **I. Spec-Driven Development First** | ✅ PASS | Spec complete and approved before planning begins; all code generated from spec via Claude Code |
| **II. AI-Native Engineering** | ✅ PASS | Plan designed for 100% code generation via Claude Code; no manual coding allowed |
| **III. Reusable Intelligence** | ✅ PASS | Architectural decisions will be documented in ADR; plan supports future MCP integration (Phase III) |
| **IV. Stateless & Cloud-Native Design** | ✅ PASS | CLI is stateless; each invocation independent. Future phases add persistence & distribution. |
| **V. Security by Default** | ✅ PASS | No secrets in code; error handling explicit; user isolation via single-user assumption |
| **VI. Incremental Evolution** | ✅ PASS | Phase I isolated (in-memory only); cleanly builds toward Phase II (database + web) |
| **VII. Production Realism** | ✅ PASS | Error handling, clear contracts, observable behavior; CLI design reflects real-world patterns |

**GATE RESULT: ✅ PASS – Plan complies with all 7 constitutional principles.**

## Project Structure

### Documentation (this feature)

```
specs/001-core-todos/
├── spec.md                      # Complete feature specification (approved)
├── plan.md                      # This file (architecture & design)
├── tasks.md                     # TDD task breakdown (to be generated by /sp.tasks)
├── checklists/
│   └── requirements.md          # Quality validation checklist
└── contracts/                   # API contracts (TBD during implementation)
```

### Source Code (repository root)

```
todo-app/
├── src/
│   ├── main.py                  # CLI entry point (argparse-based command router)
│   ├── models/
│   │   └── task.py              # Task data class (immutable-friendly)
│   ├── services/
│   │   └── task_manager.py      # TaskManager class (in-memory CRUD logic)
│   └── cli/
│       └── commands.py          # Command handlers (add, list, update, delete, complete)
│
├── tests/
│   ├── test_task_model.py       # Task data model tests (unit)
│   ├── test_task_manager.py     # TaskManager CRUD logic tests (unit)
│   ├── test_cli_commands.py     # CLI command parsing & routing tests (integration)
│   └── test_acceptance.py       # End-to-end acceptance tests (all 17 scenarios from spec)
│
├── pyproject.toml               # Python project metadata (UV package manager)
├── README.md                    # Setup and usage instructions
├── CLAUDE.md                    # Claude Code guidance for this module
└── .env.example                 # Environment variable template (none required for Phase I)
```

**Structure Decision**: Single-project CLI application (Option 1) selected because:
- Phase I is standalone, no database or web components
- Clear separation of concerns: models → services → CLI
- Easy to expand in Phase II (add backend/ and frontend/ at repo root alongside todo-app/)
- Tests colocated with source for development velocity

## Architecture & Design

### High-Level Data Flow

```
User Input
    ↓
CLI Layer (argparse, command routing)
    ↓
Command Handler (e.g., add_command, list_command)
    ↓
TaskManager Service (CRUD operations)
    ↓
Task Data Model (in-memory list)
    ↓
Output Formatter (human-readable + JSON)
    ↓
User Output (stdout/stderr)
```

### Core Components

#### 1. Task Model (`src/models/task.py`)
- **Responsibility**: Represent a single task with all attributes from spec
- **Key Attributes**:
  - `id`: int (unique, sequential, auto-generated)
  - `title`: str (required, max 256 chars)
  - `description`: str (optional, max 1024 chars, default "")
  - `status`: Enum('incomplete', 'complete') (default: 'incomplete')
  - `created_at`: datetime (ISO 8601 format)
- **Behavior**: Immutable by design; creation via factory method ensures valid state
- **Used By**: TaskManager service, CLI output formatting

#### 2. TaskManager Service (`src/services/task_manager.py`)
- **Responsibility**: In-memory CRUD operations and task lifecycle management
- **In-Memory Storage**: Private list attribute `_tasks`
- **Public Methods**:
  - `add_task(title: str, description: str = "") -> Task`: Create & return new task
  - `list_tasks() -> List[Task]`: Return all tasks (copy to prevent mutation)
  - `get_task(task_id: int) -> Task | None`: Fetch task by ID
  - `update_task(task_id: int, title: str | None, description: str | None) -> bool`: Update & return success
  - `delete_task(task_id: int) -> bool`: Delete by ID & return success
  - `toggle_task_status(task_id: int) -> bool`: Toggle complete/incomplete & return success
- **Error Handling**: Returns None or False for invalid operations; no exceptions
- **Thread Safety**: Not required for Phase I (single-threaded, single-user)

#### 3. CLI Layer (`src/cli/commands.py`)
- **Responsibility**: Parse user commands and delegate to TaskManager
- **Command Structure**: `todo <command> [args]`
- **Supported Commands**:
  - `add <title> [--description <desc>]`: Add new task
  - `list [--format json|human]`: List all tasks (human-readable by default)
  - `update <id> [--title <new_title>] [--description <new_desc>]`: Update task
  - `delete <id>`: Delete task
  - `complete <id>`: Mark task complete
  - `incomplete <id>`: Mark task incomplete
  - `help`: Display available commands
- **Input Validation**: Check task IDs are numeric, titles non-empty, args present
- **Error Messages**: Clear, actionable messages (e.g., "Task ID 99 not found. Valid IDs: 1, 2, 3")
- **Output Formatting**: Default human-readable; optional JSON for scripting

#### 4. Entry Point (`src/main.py`)
- **Responsibility**: Application bootstrap and command routing
- **Initialization**:
  - Create TaskManager instance (lasts for one invocation)
  - Parse command-line arguments via argparse
  - Route to appropriate command handler
- **Execution Model**: Single-invocation; tasks lost upon exit (in-memory expected)
- **Exit Codes**: 0 (success), 1 (invalid command), 2 (validation error)

### Why In-Memory Storage?

In-memory storage is appropriate for Phase I because:
1. **Spec Requirement**: Explicitly stated in constraints (no database, no file persistence)
2. **Simplicity**: Eliminates database setup/teardown, file I/O, schema migrations
3. **Foundation**: Phase I proves CLI design before adding persistence (Phase II)
4. **Stateless Design**: Each invocation is independent; no server state to manage
5. **Testing**: Easier to test without mocking file/DB I/O
6. **Transition**: Phase II adds database layer; Phase I code becomes reusable services

## Key Architectural & Design Decisions

The following decisions are architecturally significant and will be documented in an ADR after plan approval.

### Decision 1: Task ID Generation Strategy

**Problem**: How should task IDs be assigned to ensure uniqueness and stability?

**Options Considered**:

| Option | ID Source | Approach | Tradeoffs |
|--------|-----------|----------|-----------|
| **A: Sequential Integer (SELECTED)** | Internal counter | Increment counter on each task creation; start at 1 | ✅ Simple, predictable, testable; ❌ IDs reset on app exit (acceptable for Phase I); resets prevent distributed scaling (solved in Phase IV with UUID) |
| B: UUID | Random generation | Generate UUID v4 on each task creation | ✅ Globally unique, stable across sessions; ❌ More complex, longer IDs, harder to debug |
| C: Timestamp + Counter | Hybrid | Combine timestamp with counter (e.g., 1609459200-001) | ✅ Sortable, unique; ❌ More complex, tied to system clock |

**Selected Option**: A (Sequential Integer)
- **Rationale**: Matches spec requirement ("unique sequential ID"); simplest implementation; acceptable data loss on exit (in-memory expectation); migration path to UUIDs exists in Phase II
- **Implementation**: TaskManager maintains internal counter; increments before each task creation

### Decision 2: In-Memory Data Structure

**Problem**: Which Python data structure should store tasks?

**Options Considered**:

| Option | Structure | Lookup | Deletion | Iteration | Tradeoffs |
|--------|-----------|--------|----------|-----------|-----------|
| **A: List of Task Objects (SELECTED)** | `List[Task]` | O(n) by linear search | O(n) remove by value | O(n) fast | ✅ Simple, preserves insertion order; ❌ Linear lookup (acceptable ~10K tasks) |
| B: Dictionary by ID | `Dict[int, Task]` | O(1) by ID | O(1) by ID | O(n) items() | ✅ O(1) lookup/delete; ❌ Loses insertion order (Python 3.7+ preserves, but semantics muddy) |
| C: Index + List | Dual: `List[Task]` + `Dict[int, Task]` | O(1) or O(n) | O(1) or O(n) | O(n) | ✅ Fast lookup and delete; ❌ Complexity, dual maintenance burden |

**Selected Option**: A (List of Task Objects)
- **Rationale**: Simplicity for Phase I; O(n) acceptable for ~10K tasks (typical limit <100); Phase II/IV upgrade to indexed structure as needed
- **Implementation**: TaskManager stores `List[Task]` internally; uses list comprehension for filtering/searching

### Decision 3: CLI Command Style

**Problem**: How should users interact with the CLI?

**Options Considered**:

| Option | Style | Example | Tradeoffs |
|--------|-------|---------|-----------|
| **A: Command-based (SELECTED)** | `todo <command> <args>` | `todo add Buy groceries --description "Milk, eggs"` | ✅ Scriptable, familiar (git-like), batch-friendly; ❌ Requires command-line arg parsing |
| B: Menu-driven | Interactive loop with numbered options | Select option → provide input → repeat | ✅ User-friendly for beginners; ❌ Hard to script, verbose for power users |
| C: REPL | Read-Eval-Print-Loop | `> add Buy groceries` then `> list` | ✅ Interactive, persistent; ❌ Requires session management (out of scope) |

**Selected Option**: A (Command-based)
- **Rationale**: Matches spec's CLI interface requirement; supports automation; aligns with Phase II API design (commands → endpoints); easier to test
- **Implementation**: argparse for command parsing; subparsers for add/list/update/delete/complete/incomplete

### Decision 4: Error Handling Approach

**Problem**: How should the app handle invalid input and errors?

**Options Considered**:

| Option | Mechanism | Example | Tradeoffs |
|--------|-----------|---------|-----------|
| **A: Return Values + Messages (SELECTED)** | Functions return bool/None; CLI prints descriptive message | `update_task()` returns False; CLI prints "Task ID 99 not found" | ✅ No exceptions in main flow; ❌ Requires explicit checking |
| B: Exceptions | Raise custom exceptions; catch in main | `raise TaskNotFoundError`; catch & format | ✅ Idiomatic Python; ❌ Stack traces complicate user experience |
| C: Result Objects | Return (success: bool, message: str) tuple | `(False, "Task ID 99 not found")` | ✅ Explicit, clean; ❌ More verbose |

**Selected Option**: A (Return Values + Messages)
- **Rationale**: Simple, predictable error flow; CLI maintains full control over output; no stack traces in user-facing output
- **Implementation**: TaskManager methods return bool (success/failure); TaskManager methods optionally return error reasons; CLI formats messages

### Decision 5: Status Representation

**Problem**: How should task completion status be represented?

**Options Considered**:

| Option | Representation | List Display | Update | Tradeoffs |
|--------|-----------------|--------------|--------|-----------|
| **A: Enum (SELECTED)** | Python `Enum('incomplete', 'complete')` | `✔ [complete]` or `✘ [incomplete]` | Toggle method | ✅ Type-safe, testable; ❌ Requires Enum import |
| B: Boolean | `completed: bool` (False=incomplete, True=complete) | `[x]` or `[ ]` | `toggle_complete()` | ✅ Simple; ❌ Semantically less clear |
| C: String | `status: 'incomplete' \| 'complete'` | `[COMPLETE]` or `[PENDING]` | Conditional logic | ✅ JSON-friendly; ❌ Requires validation |

**Selected Option**: A (Enum)
- **Rationale**: Type-safe; explicit state values prevent invalid states; testable; matches spec's language ("complete"/"incomplete")
- **Implementation**: `from enum import Enum; class TaskStatus(Enum): INCOMPLETE = 'incomplete'; COMPLETE = 'complete'`

## Implementation Workflow

### Phase A: Foundation & Environment Setup

**Objective**: Prepare development environment and repository structure.

**Tasks**:
1. Verify Python 3.13+ is installed (instructions for Windows/WSL 2, macOS, Linux)
2. Initialize UV package manager (create pyproject.toml with Python 3.13 constraint)
3. Create project directory structure (src/, tests/)
4. Set up minimal README.md with setup/run instructions
5. Create CLAUDE.md with Claude Code guidance for this module

**Success Criteria**:
- Repository structure matches documented layout
- `python --version` returns 3.13+
- `uv --version` works
- Empty src/ and tests/ directories created

**Estimated Effort**: Automated setup (5-10 minutes manual)

### Phase B: Data Models & Core Logic

**Objective**: Define Task model and TaskManager service.

**Tasks**:
1. Create `src/models/task.py` with Task data class
   - Attributes: id, title, description, status (Enum), created_at
   - Factory method: `Task.create(title, description="")` (auto-generates ID)
   - String representation: `__str__()` for human-readable output

2. Create `src/services/task_manager.py` with TaskManager class
   - Private `_tasks` list
   - Private `_next_id` counter
   - Public methods: add_task, list_tasks, get_task, update_task, delete_task, toggle_task_status
   - All methods use return values (not exceptions) for error handling

3. Create unit tests:
   - `tests/test_task_model.py`: Task creation, attributes, string representation
   - `tests/test_task_manager.py`: CRUD operations, ID uniqueness, error handling

**Success Criteria**:
- Task model created with all spec attributes
- TaskManager supports full CRUD
- Unit tests pass (100% coverage of models/services)
- TaskManager maintains correct IDs (sequential, unique)

**Estimated Effort**: 30-45 minutes (model definition + service logic + tests)

### Phase C: CLI Interface & Command Routing

**Objective**: Build command-line interface using argparse.

**Tasks**:
1. Create `src/cli/commands.py` with command handlers
   - `add_command()`: Parse --description, call TaskManager.add_task(), print confirmation
   - `list_command()`: Call TaskManager.list_tasks(), format output (human + JSON)
   - `update_command()`: Parse --title/--description, call TaskManager.update_task(), print status
   - `delete_command()`: Parse task ID, call TaskManager.delete_task(), print status
   - `complete_command()`: Parse task ID, call TaskManager.toggle_task_status(id, 'complete'), print status
   - `incomplete_command()`: Parse task ID, call TaskManager.toggle_task_status(id, 'incomplete'), print status

2. Create `src/main.py` entry point
   - Initialize TaskManager (in-process)
   - Set up argparse with subparsers for each command
   - Route parsed args to appropriate command handler
   - Return appropriate exit codes (0, 1, 2)

3. Create integration tests:
   - `tests/test_cli_commands.py`: Command parsing, routing, output formatting
   - `tests/test_acceptance.py`: All 17 acceptance scenarios from spec (end-to-end)

**Success Criteria**:
- All 6 commands (add, list, update, delete, complete, incomplete) functional
- Help message lists available commands
- JSON output works (--format json flag)
- Error messages clear and actionable
- CLI integration tests pass
- All acceptance scenarios from spec pass

**Estimated Effort**: 45-60 minutes (CLI parsing + command handlers + integration tests)

### Phase D: Validation & Finalization

**Objective**: Validate against spec and prepare for handoff.

**Tasks**:
1. Manual testing of all 5 user stories
   - P1: Add task, list tasks, mark complete (MVP validation)
   - P2: Update task, delete task (enhancement validation)

2. Run all tests (unit + integration + acceptance)
   - Verify test coverage ≥ 85%
   - Ensure no flaky tests

3. Verify success criteria from spec
   - ✓ All 5 features work without errors
   - ✓ Typical 3-task workflow in <10 seconds
   - ✓ Error messages clear and actionable
   - ✓ Edge cases handled gracefully
   - ✓ All code AI-generated (no manual edits)
   - ✓ TDD discipline enforced (tests written first)

4. Create documentation
   - Complete README.md with setup and usage examples
   - Document CLAUDE.md guidance for future developers
   - Update CLAUDE.md at root to reference Phase I documentation

5. Prepare commit and PR
   - Commit with message: "feat: implement Phase I in-memory todo CLI (all acceptance criteria pass)"
   - Create PHR for plan stage

**Success Criteria**:
- All tests pass (unit, integration, acceptance)
- All 5 user stories validated manually
- No manual code edits since generation
- Documentation complete
- Ready for Phase II planning

**Estimated Effort**: 15-30 minutes (testing + validation + documentation)

## Testing & Validation Strategy

### Test-Driven Development (TDD) Workflow

Phase I enforces strict TDD discipline:

1. **Test-First**: Write acceptance tests from spec scenarios BEFORE implementing features
2. **Red Phase**: Tests fail (no implementation yet)
3. **Green Phase**: Implement minimal code to pass tests
4. **Refactor**: Clean up code while keeping tests green

### Validation Points

#### Unit Tests (models/ + services/)
- Task creation: attributes set correctly, ID unique/sequential
- TaskManager CRUD: add, list, get, update, delete all work
- Status toggling: complete ↔ incomplete transitions
- Error cases: get/update/delete on non-existent IDs

**File**: `tests/test_task_model.py`, `tests/test_task_manager.py`
**Target Coverage**: ≥ 95% of models/ and services/ code

#### Integration Tests (CLI + commands)
- Command parsing: all commands parse correctly
- Argument handling: optional arguments (--description, --format, --title)
- Command routing: correct handler called for each command
- Output formatting: human-readable + JSON output correct

**File**: `tests/test_cli_commands.py`
**Target Coverage**: ≥ 90% of CLI layer

#### Acceptance Tests (end-to-end)
- All 17 acceptance scenarios from spec.md
- User Story 1 (Add): 4 scenarios
- User Story 2 (View): 5 scenarios
- User Story 3 (Complete): 4 scenarios
- User Story 4 (Update): 4 scenarios
- User Story 5 (Delete): 3 scenarios
- Edge cases: 5 scenarios

**File**: `tests/test_acceptance.py`
**Success Criteria**: 100% pass rate

### Manual Validation Checklist

After all automated tests pass:

- [ ] `todo add Buy groceries` → Task created with ID 1, status incomplete
- [ ] `todo add Call mom --description "This weekend"` → Task created with both title and description
- [ ] `todo list` → Both tasks display with IDs, titles, and status indicators
- [ ] `todo complete 1` → Task 1 status changes to complete; list reflects change immediately
- [ ] `todo update 1 --title "Buy groceries and cook dinner"` → Task 1 title updated
- [ ] `todo incomplete 1` → Task 1 status reverts to incomplete
- [ ] `todo delete 2` → Task 2 removed; list shows only Task 1
- [ ] `todo complete 99` → Error: "Task ID 99 not found. Valid IDs: 1"
- [ ] `todo list --format json` → JSON output with all tasks and attributes
- [ ] `todo help` → Lists available commands and usage

## Architectural Decisions to Document (ADR)

The following decisions are significant and will be documented in Architecture Decision Records (ADR):

1. **ADR-001: Sequential Integer Task IDs**
   - Decision: Use sequential integers (1, 2, 3, ...) for task IDs
   - Tradeoff: Reset on app exit (acceptable for Phase I); upgrade to UUID in Phase II for distributed system

2. **ADR-002: List-Based In-Memory Storage**
   - Decision: Store tasks in Python list; O(n) linear search
   - Tradeoff: Simplicity vs. performance; acceptable for ~10K tasks

3. **ADR-003: Command-Based CLI Interaction**
   - Decision: git-like command style (`todo add ...`) vs. menu-driven
   - Tradeoff: Scriptability vs. interactivity; chosen for Phase II API alignment

4. **ADR-004: Return-Value Error Handling**
   - Decision: No exceptions; return bool/None; CLI formats messages
   - Tradeoff: Verbosity vs. clean user output; no stack traces in user experience

5. **ADR-005: Enum-Based Status Representation**
   - Decision: Use Python Enum for task status (INCOMPLETE, COMPLETE)
   - Tradeoff: Type safety vs. simplicity; enables Phase II schema evolution

## Dependencies & External Constraints

### Required Tools
- **Python 3.13+**: Language runtime
- **UV**: Package manager (included in Hackathon environment)
- **pytest**: Test framework (industry standard, installed via UV)
- **Standard Library**: argparse, json, datetime, dataclasses, enum (all built-in)

### No External Dependencies
- No Django, FastAPI, or web frameworks (Phase I is CLI)
- No ORM or database drivers (in-memory only)
- No third-party CLI libraries (argparse is sufficient)
- No package manager beyond UV

### Platform Support
- Linux: Full support (primary development platform)
- macOS: Full support
- Windows (WSL 2): Full support; native Windows CMD/PowerShell works but WSL 2 recommended
- Python 3.13+: Required; older versions not supported

## Risk Analysis & Mitigation

| Risk | Impact | Likelihood | Mitigation |
|------|--------|------------|-----------|
| Claude Code fails to generate working code | Phase I blocked | Low | Spec is comprehensive and unambiguous; iterative refinement via spec updates if needed |
| Performance issues with large task volumes | Acceptance criterion fails | Very Low | O(n) lookup acceptable for ~10K tasks; Phase II/IV add indexing if needed |
| Python version incompatibility | Build fails on some systems | Low | Test on Python 3.13.0 (latest) during Phase D; document exact version |
| Ambiguous acceptance scenarios from spec | Implementation mismatch | Very Low | Spec reviewed in detail; all 17 scenarios have concrete examples |
| Manual code edits after generation (violates AI-Native principle) | Violates constitution | Medium | Establish clear policy: regenerate from spec updates, never manual edits |

## Success Definition

Phase I is **complete** when:

1. ✅ All tests pass (unit + integration + acceptance)
2. ✅ All 17 acceptance scenarios from spec pass
3. ✅ Manual validation checklist 100% complete
4. ✅ No manual code edits (100% AI-generated via Claude Code)
5. ✅ TDD discipline enforced (tests written before implementation)
6. ✅ Code follows PEP 8 style guidelines
7. ✅ Error messages are clear and actionable
8. ✅ All 5 features work without errors in sequence
9. ✅ Typical 3-task workflow completes in <10 seconds
10. ✅ Documentation (README.md, CLAUDE.md) complete
11. ✅ Ready to commit and create PR
12. ✅ Ready for Phase II planning (database + web app)

## Next Steps

1. **Get plan approval** from stakeholders (judges/reviewers)
2. **Run `/sp.tasks`** to generate TDD task breakdown
3. **Run `/sp.implement`** to execute tasks and generate code via Claude Code
4. **Validate** all acceptance criteria pass
5. **Document decisions** in ADRs (via `/sp.adr`)
6. **Commit and create PR** (via `/sp.git.commit_pr`)
7. **Prepare for Phase II** planning

---

**Plan Status**: ✅ READY FOR IMPLEMENTATION
**Approval Gate**: Stakeholder sign-off required before `/sp.tasks`
**Risk Level**: LOW (comprehensive spec, clear requirements, proven Claude Code capabilities)
